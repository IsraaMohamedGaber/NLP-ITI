{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation\n",
        "https://spacy.io/usage/training"
      ],
      "metadata": {
        "id": "hFWePYCa82VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "#!pip install datasets\n"
      ],
      "metadata": {
        "id": "YTVnb3GSsLgY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load dataset**"
      ],
      "metadata": {
        "id": "LVWz_vla9vqs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S-TNMemr__I",
        "outputId": "eac7d0c8-e529-46cd-a718-420c49ee1ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 14041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3250\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
            "        num_rows: 3453\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map Tag IDs to Entity Labels**"
      ],
      "metadata": {
        "id": "nYQC0krR81O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = dataset['train'].features['ner_tags'].feature.names\n",
        "# Example: label_list[3] = 'B-ORG'\n"
      ],
      "metadata": {
        "id": "Nt18_kLDsJcN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert CoNLL2003 to spaCy Format"
      ],
      "metadata": {
        "id": "W2wZZM3X-oce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conll_to_spacy(dataset_split):\n",
        "    data = []\n",
        "    for item in dataset_split:\n",
        "        tokens = item[\"tokens\"]\n",
        "        tags = item[\"ner_tags\"]\n",
        "        text = \" \".join(tokens)\n",
        "        entities = []\n",
        "        offset = 0\n",
        "\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            tag_name = label_list[tag]\n",
        "            token_start = text.find(token, offset)\n",
        "            token_end = token_start + len(token)\n",
        "            offset = token_end\n",
        "\n",
        "            if tag_name.startswith(\"B-\"):\n",
        "                entity_label = tag_name[2:]\n",
        "                entities.append((token_start, token_end, entity_label))\n",
        "\n",
        "        data.append((text, {\"entities\": entities}))\n",
        "    return data\n",
        "\n",
        "TRAIN_DATA = conll_to_spacy(dataset[\"train\"].select(range(3000)))  # Use first 3K samples\n",
        "DEV_DATA = conll_to_spacy(dataset[\"validation\"])\n"
      ],
      "metadata": {
        "id": "YlXhUloG-nNt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up and Train spaCy NER Model**"
      ],
      "metadata": {
        "id": "aRlrNp05-xGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add entity labels\n",
        "for _, annotations in TRAIN_DATA:\n",
        "    for start, end, label in annotations[\"entities\"]:\n",
        "        ner.add_label(label)\n",
        "\n",
        "# Train the model\n",
        "with nlp.disable_pipes([pipe for pipe in nlp.pipe_names if pipe != \"ner\"]):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(10):  # 10 epochs\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.5))\n",
        "        for batch in batches:\n",
        "            for text, annotations in batch:\n",
        "                doc = nlp.make_doc(text)\n",
        "                example = Example.from_dict(doc, annotations)\n",
        "                nlp.update([example], sgd=optimizer, drop=0.5, losses=losses)\n",
        "        print(f\"Epoch {i+1}: Losses {losses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk3gtc46-rL9",
        "outputId": "24b43b07-fda5-4ffa-e5f6-9b8e72e44354"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Losses {'ner': np.float32(5687.4263)}\n",
            "Epoch 2: Losses {'ner': np.float32(3819.66)}\n",
            "Epoch 3: Losses {'ner': np.float32(2961.7898)}\n",
            "Epoch 4: Losses {'ner': np.float32(2471.3584)}\n",
            "Epoch 5: Losses {'ner': np.float32(2114.7102)}\n",
            "Epoch 6: Losses {'ner': np.float32(1834.8018)}\n",
            "Epoch 7: Losses {'ner': np.float32(1594.2517)}\n",
            "Epoch 8: Losses {'ner': np.float32(1483.9362)}\n",
            "Epoch 9: Losses {'ner': np.float32(1364.948)}\n",
            "Epoch 10: Losses {'ner': np.float32(1241.9075)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model**"
      ],
      "metadata": {
        "id": "h2j02_jx_IzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick evaluation on dev data\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for text, annotations in DEV_DATA[:100]:\n",
        "    doc = nlp(text)\n",
        "    predicted = set((ent.start_char, ent.end_char, ent.label_) for ent in doc.ents)\n",
        "    actual = set(annotations[\"entities\"])\n",
        "    correct += len(predicted & actual)\n",
        "    total += len(actual)\n",
        "\n",
        "print(f\"Accuracy on 100 dev samples: {correct/total:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShAMnerU_Ih_",
        "outputId": "36bc58b6-2654-46a6-bc1a-6f22a70d9ef9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on 100 dev samples: 77.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save and Use the Model**"
      ],
      "metadata": {
        "id": "h2N8QzEG--ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "nlp.to_disk(\"conll2003_ner_model\")\n",
        "# Load and test\n",
        "nlp2 = spacy.load(\"conll2003_ner_model\")\n",
        "doc = nlp2(\"Microsoft announced new plans in Paris.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymgf4WeO-33u",
        "outputId": "8e7de5cd-0ffe-4dc8-80ca-75c8f3949707"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft LOC\n",
            "Paris LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YJNKSkx_o3N"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}