{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a412a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed notebook-level 'widgets' metadata\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'cleaned_E:/israa/ITI 9 Months/NLP/Github/NLP-ITI/Notes/Day 1/NLP_lec1.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Save as cleaned notebook\u001b[39;00m\n\u001b[0;32m     22\u001b[0m cleaned_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m notebook_filename\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcleaned_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     24\u001b[0m     nbformat\u001b[38;5;241m.\u001b[39mwrite(nb, f)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Cleaned notebook saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaned_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'cleaned_E:/israa/ITI 9 Months/NLP/Github/NLP-ITI/Notes/Day 1/NLP_lec1.ipynb'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "\n",
    "# Original file\n",
    "notebook_filename = \"E:/israa/ITI 9 Months/NLP/Github/NLP-ITI/Notes/Day 1/NLP_lec1.ipynb\"\n",
    "\n",
    "# Load notebook\n",
    "with open(notebook_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "# Clean metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "    print(\"Removed notebook-level 'widgets' metadata\")\n",
    "\n",
    "for cell in nb.cells:\n",
    "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
    "        del cell[\"metadata\"][\"widgets\"]\n",
    "        print(\"Removed 'widgets' from a cell\")\n",
    "\n",
    "# Build new filename (same folder, prefixed with cleaned_)\n",
    "folder, fname = os.path.split(notebook_filename)\n",
    "cleaned_name = os.path.join(folder, \"cleaned_\" + fname)\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(cleaned_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(f\"✅ Cleaned notebook saved as {cleaned_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GnisyvCLRuHz",
   "metadata": {
    "id": "GnisyvCLRuHz"
   },
   "source": [
    "# Documentation and Resources\n",
    "\n",
    "---\n",
    "https://tiktokenizer.vercel.app/?model=gpt-4-1106-preview\n",
    "\n",
    "https://www.nltk.org/\n",
    "\n",
    "https://docs.pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0db01f",
   "metadata": {
    "id": "be0db01f"
   },
   "source": [
    "# Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563d05b",
   "metadata": {
    "id": "8563d05b"
   },
   "source": [
    "1. Lowercase Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb6115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dbb6115",
    "outputId": "d14d6b99-2e17-4a2b-c8aa-a2cec599ba45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! this is an example.\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = \"Hello World! This is an Example.\"\n",
    "lowercase_text = lowercase_text(text)\n",
    "print(lowercase_text)  # hello world! this is an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff375ac8",
   "metadata": {
    "id": "ff375ac8"
   },
   "source": [
    "2. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7dca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ae7dca1",
    "outputId": "bd18c690-9d98-44e1-fa8d-237943e13ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english')) #TAKE words\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "tokens = ['hello', 'world', 'this', 'is','The', 'an', 'example']\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(filtered_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9724",
   "metadata": {
    "id": "072a9724"
   },
   "source": [
    "3. Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b463c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e7b463c",
    "outputId": "2da5d532-7d0f-47a2-a6f9-266225ee5cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is an example with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "text = \"Hello, world! This is an example: with punctuation.\"\n",
    "clean_text = remove_punctuation(text)\n",
    "print(clean_text)  # \"Hello world This is an example with punctuation\"\n",
    "#this process help to save token\n",
    "#token = money"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0b2ae",
   "metadata": {
    "id": "c7c0b2ae"
   },
   "source": [
    "4. Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e49846",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68e49846",
    "outputId": "20085444-f81d-41bc-f7e5-f914c1960091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "# Match Simple Text\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Python is fun\" #corpus\n",
    "match = re.search(\"Python\", text)\n",
    "print(match.group())  # Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09393a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f09393a3",
    "outputId": "b4c6c768-c27b-4894-d75c-7361acf72e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern not found\n"
     ]
    }
   ],
   "source": [
    "# Match Simple Text\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Python is fun\"#corpus\n",
    "match = re.search(\"love\", text)\n",
    "# Check if a match was found before calling .group()\n",
    "if match:\n",
    "    print(match.group())\n",
    "else:\n",
    "    print(\"Pattern not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dd6e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa5dd6e2",
    "outputId": "84a0e06d-fd18-4295-9cf9-0f882cd72897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "amazing\n"
     ]
    }
   ],
   "source": [
    "# Match Beginning and End\n",
    "\n",
    "# ^ matches start of string, $ matches end\n",
    "text = \"Python is amazing\"\n",
    "start_match = re.search(\"^Python\", text)\n",
    "print(start_match.group())  # Python\n",
    "\n",
    "end_match = re.search(\"amazing$\", text)\n",
    "print(end_match.group())  # amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2052a5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2052a5f",
    "outputId": "7eff2bab-bd98-46a7-8df5-20db96fb33b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '3', '5']\n",
      "['3', '35']\n"
     ]
    }
   ],
   "source": [
    "# Match Digits\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "digits = re.findall(r\"\\d\", text)  # r prefix creates a raw string # d refer to digit\n",
    "print(digits)  # ['3', '3', '5']\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "# \\d+ matches one or more digits\n",
    "numbers = re.findall(r\"\\d+\", text)\n",
    "print(numbers)  # ['3', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea881a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fea881a5",
    "outputId": "f5e76562-2346-4c8a-aba3-ef5ac8b0a120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_123', 'has', 'logged', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Match Word Characters\n",
    "\n",
    "text = \"user_123 has logged in\" #corpus\n",
    "# \\w matches alphanumeric + underscore\n",
    "word_chars = re.findall(r\"\\w+\", text) #w+ refer to words\n",
    "print(word_chars)  # ['user_123', 'has', 'logged', 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a8524",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed8a8524",
    "outputId": "5e70ced2-a0d1-421d-de17-d538dbede421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'colour']\n"
     ]
    }
   ],
   "source": [
    "# matching zero or more\n",
    "\n",
    "text = \"color colour colouur\"\n",
    "pattern = re.findall(r\"colou?r\", text)  # ? means 0 or 1 of previous character\n",
    "print(pattern)  # ['color', 'colour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36dce7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa36dce7",
    "outputId": "437a5133-d089-40cd-c237-d8700b53fe91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loooove']\n"
     ]
    }
   ],
   "source": [
    "# Match One or More\n",
    "\n",
    "text = \"I loooove Python\"\n",
    "pattern = re.findall(r\"lo+ve\", text)  # + means 1 or more of previous character\n",
    "print(pattern)  # ['loooove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a9f41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b81a9f41",
    "outputId": "17fe2ce2-d80f-46ab-d4fe-1b197b8b0403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555-1234', '678-5678']\n"
     ]
    }
   ],
   "source": [
    "# Match Exact Number\n",
    "\n",
    "\n",
    "text = \"Phone numbers: 555-1234 and 555678-5678\"\n",
    "pattern = re.findall(r\"\\d{3}-\\d{4}\", text)  # {n} means exactly n occurrences\n",
    "print(pattern)  # ['555-1234', '555-5678']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118cc471",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "118cc471",
    "outputId": "1378c024-6f47-4eca-f6b9-2b85c201b80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555-1234', '678-5678']\n"
     ]
    }
   ],
   "source": [
    "# Match Exact Number\n",
    "\n",
    "\n",
    "text = \"Phone numbers: 555-1234 and 555678-56789\"\n",
    "pattern = re.findall(r\"\\d{3}-\\d{4}\", text)  # {n} means exactly n occurrences\n",
    "print(pattern)  # ['555-1234', '555-5678']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484cf2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a484cf2f",
    "outputId": "32733ee5-6494-464f-adb1-54f19f8a7d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# Match Any of Several Characters\n",
    "\n",
    "text = \"The cat and the rat sat on the mat\"\n",
    "pattern = re.findall(r\"[cr]at\", text)  # matches 'cat' or 'rat'\n",
    "print(pattern)  # ['cat', 'rat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0bea2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfb0bea2",
    "outputId": "3848cbaa-3f36-49d3-fbc6-ec1c6efb4cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['D', 'E']\n",
      "['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']\n"
     ]
    }
   ],
   "source": [
    "# Match Range of Characters\n",
    "\n",
    "text = \"a1b2c3D4E5\"\n",
    "letters = re.findall(r\"[a-z]\", text)  # lowercase letters\n",
    "print(letters)  # ['a', 'b', 'c']\n",
    "\n",
    "uppercase = re.findall(r\"[A-Z]\", text)  # uppercase letters\n",
    "print(uppercase)  # ['D', 'E']\n",
    "\n",
    "alphanumeric = re.findall(r\"[a-zA-Z0-9]\", text)  # all alphanumeric\n",
    "print(alphanumeric)  # ['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd32b52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b52",
    "outputId": "259a0a2a-3c5a-4d33-e54c-5eb1dc188344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user@example.com is valid\n",
      "invalid@email is invalid\n",
      "name.last@domain.co.uk is valid\n"
     ]
    }
   ],
   "source": [
    "# Email Validation\n",
    "\n",
    "emails = [\"user@example.com\", \"invalid@email\", \"name.last@domain.co.uk\"]\n",
    "pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "for email in emails:\n",
    "    if re.match(pattern, email):\n",
    "        print(f\"{email} is valid\")\n",
    "    else:\n",
    "        print(f\"{email} is invalid\")\n",
    "# user@example.com is valid\n",
    "# invalid@email is invalid\n",
    "# name.last@domain.co.uk is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e73f45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8e73f45",
    "outputId": "5a80eb30-76ac-47d5-b59e-9014b8dc406c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  apples and  oranges.\n",
      "There are NUM apples and NUM oranges.\n"
     ]
    }
   ],
   "source": [
    "# Number Removal/Normalization\n",
    "# /d is for digits\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_numbers(text, replacement='NUM'):\n",
    "    return re.sub(r'\\d+', replacement, text)\n",
    "\n",
    "text = \"There are 123 apples and 456 oranges.\"\n",
    "text_no_numbers = remove_numbers(text)\n",
    "text_normalized = replace_numbers(text)\n",
    "\n",
    "print(text_no_numbers)  # \"There are  apples and  oranges.\"\n",
    "print(text_normalized)  # \"There are NUM apples and NUM oranges.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a85c55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73a85c55",
    "outputId": "1f05b929-c3de-4c9c-b493-a5d04e5f7e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters unicode like  should be removed\n"
     ]
    }
   ],
   "source": [
    "# Noise Removal\n",
    "import re\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove special characters and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove ASCII/Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text = \"Special @#! characters & unicode like 你好 should be removed.\"\n",
    "clean_text = remove_noise(text)\n",
    "print(clean_text)  # \"Special characters  unicode like  should be removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3950e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ade3950e",
    "outputId": "a6ff3df0-442f-49d3-fc25-0ea4f50e1c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact us at [EMAIL] or visit [URL] or call [PHONE]\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization with REGEX\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs\n",
    "    #\\S means \"non-whitespace character\" (the opposite of \\s)\n",
    "    # \\s = whitespace (spaces, tabs, newlines)\n",
    "    # \\S = any character that is NOT whitespace\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "\n",
    "    # Replace phone numbers\n",
    "    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n",
    "\n",
    "    # Replace multiple whitespaces with single\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Replace elongated words (e.g., \"hellooooo\" -> \"hello\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "text = \"Contact us at example@gmail.com or visit https://example.com or call 123-456-7890\"\n",
    "normalized_text = normalize_text(text)\n",
    "print(normalized_text)  # \"contact us at [EMAIL] or visit [URL] or call [PHONE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62580390",
   "metadata": {
    "id": "62580390"
   },
   "source": [
    "5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ca099",
   "metadata": {
    "id": "d92ca099"
   },
   "source": [
    "Tokenization is the process of splitting text into smaller pieces, called tokens.\n",
    "These tokens can be:\n",
    "\n",
    "Words → Word-level tokenization\n",
    "\n",
    "Characters → Character-level tokenization\n",
    "\n",
    "Subwords → Subword-level tokenization (used in models like BERT, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce6f50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bce6f50",
    "outputId": "8ade1a35-fd80-46c6-c7d4-40d779fe1819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
      "['Hello world.', 'How are you today?']\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK\n",
    "import nltk\n",
    "# This line was already present, but might not have downloaded punkt_tab\n",
    "# nltk.download('punkt')\n",
    "# Download the specific missing resource\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Word tokenization\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    # Sentence tokenization\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    return word_tokens, sentence_tokens\n",
    "\n",
    "text = \"Hello world. How are you today?\"\n",
    "word_tokens, sentence_tokens = tokenize_text(text)\n",
    "print(word_tokens)  # ['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
    "print(sentence_tokens)  # ['Hello world.', 'How are you today?']\n",
    "\n",
    "\n",
    "# we can look at letters as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12ab27",
   "metadata": {
    "id": "ef12ab27"
   },
   "source": [
    "Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ce5d4",
   "metadata": {
    "id": "410ce5d4"
   },
   "outputs": [],
   "source": [
    "#pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0294e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13a0294e",
    "outputId": "cbdb7811-77b8-425a-f93f-8d4628af5d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'how', 'are', 'you', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Loads the tokenizer for the BERT model, specifically the uncased version (which means it converts all text to lowercase and ignores case differences).\n",
    "# This model is widely used by developers for processing English text.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Tokenizes the sentence \"Hello world. How are you today?\" into smaller units (tokens) that are understandable by the model.\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ab40b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "11bc99cc7fb34285bf5af9f25143cecd",
      "f4cb8ebd49294ad4b060d3b178bbb917",
      "2b6364bb2b8242099499346723518a41",
      "103842c61e3d43ba8002e9ab80dce900",
      "d6d32d075a1a4a5e8151170628d9cfa4",
      "f0868dd66f1c4addb1454e2dd1c9d6cd",
      "f55c9e4409f1462c90704678fa075e73",
      "7fd80d0561a34a269925c267a73b574b",
      "c35c51fb08a14c3ca5422327b48d2815",
      "214447fab23a47dab884278bcfb1d282",
      "ce76bac0ecb74f038509734d8c03abd5",
      "002fccd754a54b38af69bcac8d5750f3",
      "9ac1446f7c594b0dbef7625f28a14330",
      "21c947e09ea84a97ba50de5de1c58e07",
      "729f976e59574d128734da27e46fd613",
      "b34fc2212fbd49a0b6b4fc551cf54729",
      "6a70bcf0314d47f29f4934cd5598533a",
      "399fe5a43dca44079053f6dd868ae62a",
      "c236546471b746c693f8c90d98cea850",
      "3df5248e1c8c499286c8dc305d43ed00",
      "0e76c67604a8464c847ef7daadb9d8c6",
      "7b00edb156554392a8c3d8791ab9c9cd",
      "57eba6cb4835475b88e6f9acd064bde4",
      "8f82b400c7084b46a4f36804df4e3d6c",
      "ca1d275078ea403da3d0884a5e624054",
      "fb992c71dd41429eabca4f9fee23178f",
      "081ef134a7c2489ea0ddbfc39130cd75",
      "5fc78df99bb04f09b8ec036c84562d05",
      "a4933510dd5a4a229cb9137c355c0719",
      "a27e8ba3eac0457ea2d8b0e5789fb7f5",
      "0c579a0f63ce40a29bd99a26e3c74f73",
      "4f2778b7f315472f86078a937d570897",
      "0800cc787ceb499ca4102a3526cdd656",
      "8d36af0987fe4908bcb97d12477130fd",
      "2f1ec995aa12402e9ac2bf51f4c1ffb5",
      "5694b6a763804bb89ab04c3b11950468",
      "9da3c79e9c3945e19c42e1d8fc583111",
      "0d52f7116e4247d382716cdb87191d9e",
      "d8bda64ca8864ec9b2a4c708d732261a",
      "cc06afdb420b490588f78b479876ad52",
      "1b52217528dc4845bb4d14bae1712f96",
      "9fa3a14f45fc49efa7592346abf60b31",
      "92cb2c87bc044d5c965a7be39bfdb696",
      "28f069c994094621809d9b40b77aeb0d",
      "447f02b98c374402af86f268a7e3d740",
      "70dfa399e1a94557a8e2fc5c1de0439a",
      "56c0ad4443964f65b4bd44422f214202",
      "a1163da172204074a9684031324de871",
      "471a66dd04f943ba80b8ccc8a04fad81",
      "9fc34e5cef8741738374c58d14441c84",
      "0c22183693c948db9d7727ba44e75096",
      "a95deb52410f4f398d4fefbc425d540a",
      "5b6499cb242f4889a7b4cc2c45708139",
      "16bc25c69e5b47019f688ac524948897",
      "83be60d6bc6f491f94c204a71e7ad819"
     ]
    },
    "id": "cc8ab40b",
    "outputId": "93bc7b22-2953-4014-9b48-08f52949ad51"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bc99cc7fb34285bf5af9f25143cecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002fccd754a54b38af69bcac8d5750f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eba6cb4835475b88e6f9acd064bde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36af0987fe4908bcb97d12477130fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447f02b98c374402af86f268a7e3d740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ġworld', '.', 'ĠHow', 'Ġare', 'Ġyou', 'Ġtoday', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n",
    "\n",
    "# The Ġ symbol represents a space before the word (tokenized using byte-level BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6c0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efe6c0f5",
    "outputId": "daa123a8-da24-454c-e651-a25037e9a9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', 'bel', 'iev', 'ability']\n",
      "['ban', 'ana', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"unbelievability\")\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(\"banana \")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5bcfd",
   "metadata": {
    "id": "6fe5bcfd"
   },
   "source": [
    "6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bbb75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "930bbb75",
    "outputId": "6752c16e-faf7-4fb4-a164-3377477d2827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: ['run', 'run', 'ran', 'easili', 'fairli']\n",
      "Lancaster: ['run', 'run', 'ran', 'easy', 'fair']\n",
      "Snowball: ['run', 'run', 'ran', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "def stem_words(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "\n",
    "    porter_stems = [porter.stem(token) for token in tokens]\n",
    "    lancaster_stems = [lancaster.stem(token) for token in tokens]\n",
    "    snowball_stems = [snowball.stem(token) for token in tokens]\n",
    "\n",
    "    return porter_stems, lancaster_stems, snowball_stems\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'easily', 'fairly']\n",
    "porter_stems, lancaster_stems, snowball_stems = stem_words(tokens)\n",
    "print(f\"Porter: {porter_stems}\")    # ['run', 'run', 'ran', 'easili', 'fairli']\n",
    "print(f\"Lancaster: {lancaster_stems}\")  # ['run', 'run', 'ran', 'easy', 'fair']\n",
    "print(f\"Snowball: {snowball_stems}\")    # ['run', 'run', 'ran', 'easili', 'fair']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86d4ed",
   "metadata": {
    "id": "6b86d4ed"
   },
   "source": [
    "7. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16445e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd16445e",
    "outputId": "ce51a099-c2d9-4fd3-8158-671cb4673e6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'run', 'ran', 'better', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'better', 'mice']\n",
    "lemmatized_tokens = lemmatize_words(tokens)\n",
    "print(lemmatized_tokens)  # ['running', 'run', 'ran', 'better', 'mouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f4cf1",
   "metadata": {
    "id": "561f4cf1"
   },
   "source": [
    "8. Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2adb167",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2adb167",
    "outputId": "990e0aa4-a21d-4416-c99a-1da64ac42fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.2\n",
      "['help', 'world', 'example', 'apple']\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspellchecker\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    corrected = [spell.correction(token) for token in tokens]\n",
    "    return corrected\n",
    "\n",
    "tokens = ['helo', 'wrld', 'example','appble']\n",
    "corrected_tokens = correct_spelling(tokens)\n",
    "print(corrected_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaff4a",
   "metadata": {
    "id": "f9eaff4a"
   },
   "source": [
    "9. Text Normalization with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb885728",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb885728",
    "outputId": "ce481ed9-02c3-4f9a-ad5d-080901b4eba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: The quick brown fox jumped over the lazy dog.\n",
      "Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Noun phrases: ['brown fox jumpd', 'lazzy dog']\n"
     ]
    }
   ],
   "source": [
    "#!pip install textblob\n",
    "#!python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def normalize_with_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Correct spelling\n",
    "    corrected = blob.correct()\n",
    "\n",
    "    # Get sentiment\n",
    "    sentiment = blob.sentiment\n",
    "\n",
    "    # Get noun phrases\n",
    "    noun_phrases = blob.noun_phrases\n",
    "\n",
    "    return str(corrected), sentiment, noun_phrases\n",
    "\n",
    "text = \"The quik brown fox jumpd over the lazzy dog.\"\n",
    "# text = \"guuod feeling\"\n",
    "corrected, sentiment, noun_phrases = normalize_with_textblob(text)\n",
    "\n",
    "print(f\"Corrected: {corrected}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Noun phrases: {noun_phrases}\")\n",
    "\n",
    "# Corrected: The quick brown fox jumped over the lazy dog.\n",
    "# Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)      =poor way in sentinel analysis\n",
    "# Noun phrases: ['brown fox jumpd', 'lazzy dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09875",
   "metadata": {
    "id": "62f09875"
   },
   "source": [
    "10. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b76c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "688b76c5",
    "outputId": "a7e5b0ae-a711-41cd-f711-d32ca3bc4da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#Named Entity Recognition instead of limatizay=tion and steming\n",
    "import spacy\n",
    "\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") #corpus or text\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "#text = \"i am eating an Apple \"\n",
    "\n",
    "entities = extract_entities(text)\n",
    "print(entities)  # [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78bd2a",
   "metadata": {
    "id": "4a78bd2a"
   },
   "source": [
    "11. Text Cleaning (HTML/XML tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b466396",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b466396",
    "outputId": "c3d97933-e0ab-40d3-8312-1c5bd93e42e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sample HTML text?.\n",
      "This is sample HTML text?.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html_text):\n",
    "    # Using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "    return clean_text\n",
    "\n",
    "def clean_html_regex(html_text):\n",
    "    # Using regex\n",
    "    clean_text = re.sub(r'<.*?>', '', html_text)\n",
    "    return clean_text\n",
    "\n",
    "html = \"<div><p>This is <b>sample</b> HTML text?.</p></div>\"\n",
    "clean_bs = clean_html(html)\n",
    "clean_re = clean_html_regex(html)\n",
    "\n",
    "print(clean_bs)  # \"This is sample HTML text.\"\n",
    "print(clean_re)  # \"This is sample HTML text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24af3bd",
   "metadata": {
    "id": "d24af3bd"
   },
   "source": [
    "12. Contractions Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559017f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1559017f",
    "outputId": "f68f6541-7cb5-4897-e5b6-54c371b7c833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot do this and I will not try it.\n"
     ]
    }
   ],
   "source": [
    " #!pip install contractions\n",
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "text = \"I can't do this and I won't try it.\"\n",
    "expanded = expand_contractions(text)\n",
    "print(expanded)  # \"I cannot do this and I will not try it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f29bb",
   "metadata": {
    "id": "708f29bb"
   },
   "source": [
    "13. Text Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565a343",
   "metadata": {
    "id": "a565a343"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def vectorize_texts(texts):\n",
    "    # Bag of Words\n",
    "    count_vec = CountVectorizer()\n",
    "    bow = count_vec.fit_transform(texts)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_vec = TfidfVectorizer()\n",
    "    tfidf = tfidf_vec.fit_transform(texts)\n",
    "\n",
    "    return bow, count_vec.get_feature_names_out(), tfidf, tfidf_vec.get_feature_names_out()\n",
    "\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "]\n",
    "\n",
    "bow, bow_features, tfidf, tfidf_features = vectorize_texts(texts)\n",
    "print(\"BoW features:\", bow_features)\n",
    "print(\"BoW matrix:\\n\", bow.toarray())\n",
    "print(\"\\nTF-IDF features:\", tfidf_features)\n",
    "print(\"TF-IDF matrix:\\n\", tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1d3b1",
   "metadata": {
    "id": "c3d1d3b1"
   },
   "source": [
    "14. Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04d775",
   "metadata": {
    "id": "ad04d775"
   },
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "# pip install --upgrade --force-reinstall --no-cache-dir numpy gensim\n",
    "# restart kernel\n",
    "import gensim.downloader as api\n",
    "\n",
    "def get_word_embeddings(words):\n",
    "    # Load pre-trained Word2Vec embeddings\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    embeddings = {}\n",
    "    for word in words:\n",
    "            embeddings[word] = model[word]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\"]\n",
    "embeddings = get_word_embeddings(words)\n",
    "\n",
    "for word, vector in embeddings.items():\n",
    "    print(f\"{word}: vector shape {vector.shape}\")\n",
    "\n",
    "# king: vector shape (300,)\n",
    "# queen: vector shape (300,)\n",
    "# man: vector shape (300,)\n",
    "# woman: vector shape (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb02608",
   "metadata": {
    "id": "4fb02608"
   },
   "source": [
    "18. Language Detection and Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cbfa9",
   "metadata": {
    "id": "3e8cbfa9"
   },
   "outputs": [],
   "source": [
    "# !pip install googletrans==4.0.0rc1 -q\n",
    "# !pip install langdetect -q\n",
    "# restart kernel\n",
    "\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "def detect_and_translate(text, target_lang='en'):\n",
    "    # Detect language\n",
    "    source_lang = detect(text)\n",
    "\n",
    "    # Translate text\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
    "\n",
    "    return source_lang, translation.text\n",
    "\n",
    "text = \"Bonjour le monde\"\n",
    "source, translation = detect_and_translate(text)\n",
    "print(f\"Detected language: {source}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "# Detected language: fr\n",
    "# Translation: Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764425",
   "metadata": {
    "id": "9d764425"
   },
   "source": [
    "19. Custom Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742d718",
   "metadata": {
    "id": "8742d718"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(texts, min_freq=2, max_vocab_size=10000):\n",
    "    # Tokenize all texts\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # Count frequency\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Filter by frequency and vocabulary size\n",
    "    vocab = {token: count for token, count in token_counts.most_common(max_vocab_size)\n",
    "             if count >= min_freq}\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    token2id = {token: idx for idx, (token, _) in enumerate(vocab.items())}\n",
    "    id2token = {idx: token for token, idx in token2id.items()}\n",
    "\n",
    "    return vocab, token2id, id2token\n",
    "\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vocab, token2id, id2token = create_vocabulary(texts, min_freq=2)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Token to ID mapping:\", token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac3c69",
   "metadata": {
    "id": "b8ac3c69"
   },
   "source": [
    "Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98771f4",
   "metadata": {
    "id": "a98771f4"
   },
   "source": [
    "20. Comprehensive Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5102de3",
   "metadata": {
    "id": "b5102de3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet'])\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "text = \"This is an example! The preprocessing pipeline removes punctuation, numbers (123), and stopwords.\"\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
